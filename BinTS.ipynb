{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from probts.data.data_utils.data_scaler import Scaler, StandardScaler, IdentityScaler, TemporalScaler, BinScaler, \\\n",
    "    BinaryQuantizer\n",
    "\n",
    "from probts.model.forecaster import LinearForecaster, NaiveForecaster\n",
    "from probts.model.forecast_module import ProbTSForecastModule\n",
    "from probts.data import ProbTSDataModule, DataManager, ProbTSBatchData\n",
    "from probts.utils import find_best_epoch\n",
    "from lightning import Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# class BinaryQuantizer(Scaler):\n",
    "#     def __init__(self, num_bins=200, min_val=-10.0, max_val=10.0):\n",
    "#         super().__init__()\n",
    "#         self.num_bins = num_bins\n",
    "#         self.min_val = min_val\n",
    "#         self.max_val = max_val\n",
    "#         self.bin_values_ = torch.linspace(self.min_val, self.max_val, self.num_bins)\n",
    "#\n",
    "#     def fit(self, values):\n",
    "#         self.min_val = values.min()\n",
    "#         self.max_val = values.max()\n",
    "#         self.bin_values_ = torch.linspace(self.min_val, self.max_val, self.num_bins)\n",
    "#\n",
    "#     def fit_transform(self, values):\n",
    "#         self.fit(values)\n",
    "#         return self.transform(values)\n",
    "#\n",
    "#     def transform(self, values):\n",
    "#         bin_thresholds = self.bin_values_.reshape(1, 1, -1)\n",
    "#         return (values >= bin_thresholds).float()\n",
    "#\n",
    "#     def inverse_transform(self, values):\n",
    "#         reversed_bin = torch.flip(values, dims=(-1,))\n",
    "#         idx_first_one_reversed = reversed_bin.argmax(axis=-1)[..., None]\n",
    "#         idx_last_one = self.num_bins - 1 - idx_first_one_reversed\n",
    "#         reconstructed = self.bin_values_[idx_last_one]\n",
    "#         return reconstructed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# class StandardBinScaler(Scaler):\n",
    "#     def __init__(self, standard: StandardScaler, bin: BinaryQuantizer):\n",
    "#         super().__init__()\n",
    "#         self.standard = standard\n",
    "#         self.bin = bin\n",
    "#\n",
    "#     def fit(self, X):\n",
    "#         Z = self.standard.fit_transform(X)\n",
    "#         self.bin.fit(Z)\n",
    "#         # print('the scaler was fitted')\n",
    "#\n",
    "#     def transform(self, X):\n",
    "#         Z = self.standard.transform(X)\n",
    "#         return self.bin.transform(Z)\n",
    "#\n",
    "#     def fit_transform(self, X):\n",
    "#         self.fit(X)\n",
    "#         return self.transform(X)\n",
    "#\n",
    "#     def inverse_transform(self, X):\n",
    "#         Z = self.bin.inverse_transform(X)\n",
    "#         return self.standard.inverse_transform(Z)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "# data_manager = DataManager(\n",
    "#     dataset='tourism_monthly',\n",
    "#     path='../datasets',\n",
    "#     context_length=12,\n",
    "#     prediction_length=12,\n",
    "# )\n",
    "# data_manager.context_length"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# class CustomDataManager(DataManager):\n",
    "#     def _configure_scaler(self, scaler_type: str):\n",
    "#         \"\"\"Configure the scaler.\"\"\"\n",
    "#         if scaler_type == \"standard\":\n",
    "#             return StandardScaler(var_specific=self.var_specific_norm)\n",
    "#         elif scaler_type == \"temporal\":\n",
    "#             return TemporalScaler()\n",
    "#         elif scaler_type == \"binary\":\n",
    "#             return BinaryQuantizer()\n",
    "#         elif scaler_type == \"normalization+binary\":\n",
    "#             # return StandardBinScaler(StandardScaler(var_specific=self.var_specific_norm), BinaryQuantizer())\n",
    "#             return BinScaler(TemporalScaler(), BinaryQuantizer())\n",
    "#         return IdentityScaler()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "# data_module = ProbTSDataModule(\n",
    "#     data_manager=data_manager,\n",
    "#     batch_size=32,\n",
    "#     test_batch_size=32,\n",
    "#     num_workers=8,\n",
    "# )\n",
    "# test_dataloader = data_module.test_dataloader()\n",
    "# train_dataloader = data_module.train_dataloader()\n",
    "# val_dataloader = data_module.val_dataloader()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# for test_batch in test_dataloader:\n",
    "#     break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "# batch_data = ProbTSBatchData(test_batch, 'cpu')\n",
    "# batch_data.past_target_cdf.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,3))\n",
    "# plt.plot(batch_data.past_target_cdf[13, :, 0].t())\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "# scaler = StandardBinScaler(StandardScaler(), BinaryQuantizer())\n",
    "# scaler.fit(batch_data.past_target_cdf)\n",
    "# transformed = scaler.transform(batch_data.past_target_cdf)\n",
    "# transformed.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,3))\n",
    "# plt.imshow(transformed[13].T, aspect='auto', interpolation='none', cmap='Reds')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# reconstructed = scaler.inverse_transform(transformed)\n",
    "# reconstructed.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,3))\n",
    "# plt.plot(reconstructed[13, :, 0].t())\n",
    "# plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Short-term Dataset: tourism_monthly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download tourism_monthly_dataset.zip:: 200kB [00:00, 489kB/s]\n",
      "creating json files: 100%|██████████| 366/366 [00:00<00:00, 307268.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No validation set is used.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_manager = DataManager(\n",
    "    dataset='tourism_monthly',\n",
    "    # dataset='m4_daily',\n",
    "    path='../datasets',\n",
    "    context_length=72,\n",
    "    prediction_length=24,\n",
    "    scaler=\"identity\",\n",
    ")\n",
    "\n",
    "# data_manager = DataManager(\n",
    "#     dataset='m4_daily',\n",
    "#     # dataset='etth1',\n",
    "#     path='./datasets',\n",
    "#     context_length=12,\n",
    "#     prediction_length=12,\n",
    "#     scaler=\"standard_binary\",\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "# data_manager.dataset_raw.training_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "72"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_manager.context_length"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "data_module = ProbTSDataModule(\n",
    "    data_manager=data_manager,\n",
    "    batch_size=1,\n",
    "    test_batch_size=1,\n",
    "    num_workers=8,\n",
    ")\n",
    "test_dataloader = data_module.test_dataloader()\n",
    "train_dataloader = data_module.train_dataloader()\n",
    "val_dataloader = data_module.val_dataloader()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "for test_batch in test_dataloader:\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 84])"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch['past_target_cdf'].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([84, 1])"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch['past_target_cdf'].reshape(-1, 1).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# data_manager.scaler.standard.mean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[5466.7803],\n        [3235.1677],\n        [2157.9800],\n        [1379.7252],\n        [1728.0400],\n        [1350.1099],\n        [1216.0149],\n        [1751.3252],\n        [1805.3201],\n        [2570.0249],\n        [3204.2402],\n        [5395.7202],\n        [6078.8286],\n        [3587.0984],\n        [2285.1951],\n        [1582.1899],\n        [1787.4298],\n        [1554.8701],\n        [1409.8649],\n        [1612.1250],\n        [2286.2400],\n        [2913.7551],\n        [3645.9084],\n        [5956.7085],\n        [6326.9751],\n        [3914.6602],\n        [2617.6750],\n        [1675.1650],\n        [2139.2200],\n        [1715.4899],\n        [1663.5800],\n        [2053.7000],\n        [2354.9299],\n        [3038.5918],\n        [3470.6094],\n        [6606.1836],\n        [6587.6367],\n        [4133.7827],\n        [2960.0244],\n        [1762.5850],\n        [2125.6401],\n        [1815.9150],\n        [1632.3149],\n        [2210.3950],\n        [2210.2151],\n        [3099.2693],\n        [3468.7778],\n        [6482.9253],\n        [6665.4849],\n        [4006.3618],\n        [2882.3350],\n        [1775.2499],\n        [2171.6499],\n        [1796.4750],\n        [1692.3499],\n        [1949.7852],\n        [2680.6301],\n        [2645.9500],\n        [3414.7429],\n        [5772.8770],\n        [6053.7041],\n        [3878.1284],\n        [2806.5149],\n        [1735.5382],\n        [2128.9199],\n        [1608.0142],\n        [1441.3301],\n        [2068.2351],\n        [2207.6101],\n        [2918.4099],\n        [3400.8179],\n        [6048.7422],\n        [6483.1401],\n        [4063.5027],\n        [2900.2300],\n        [1907.0950],\n        [2338.5100],\n        [1787.1650],\n        [1699.6451],\n        [1979.1052],\n        [2824.2600],\n        [3076.5049],\n        [3402.5850],\n        [5985.8301]])"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_manager.scaler.transform(test_batch['past_target_cdf'].reshape(-1, 1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "# for train_batch in train_dataloader:\n",
    "#     break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "# batch_data = ProbTSBatchData(test_batch, 'cpu')\n",
    "# batch_data.past_target_cdf.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "72"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_manager.context_length"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "from probts.model.forecaster.prob_forecaster.binconv import BinConv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "### h = 2, L =5\n",
    "### total: 7\n",
    "### target indexes: 5,6\n",
    "## h=0 - 0:5 [0,1,2,3,4]\n",
    "## h=1 - 1:6 [1,2,3,4,5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "# def most_probable_monotonic_sequence(p: torch.Tensor, is_sample: bool, eps: float = 1e-6):\n",
    "#     \"\"\"\n",
    "#     p: Tensor of shape (B, D) with probabilities\n",
    "#     Returns:\n",
    "#         best_sequences: Tensor of shape (B, D) with the most probable [1...1, 0...0] sequence\n",
    "#         best_probs: Tensor of shape (B,) with normalized probability of the best sequence\n",
    "#     \"\"\"\n",
    "#     B, D = p.shape\n",
    "#\n",
    "#     # Clamp p to avoid log(0) or log(1) instability\n",
    "#     p_clamped = p.clamp(min=eps, max=1 - eps)\n",
    "#\n",
    "#     # Use log domain to compute cumulative products\n",
    "#     log_p = torch.log(p_clamped)\n",
    "#     log_1_minus_p = torch.log(1 - p_clamped)\n",
    "#\n",
    "#     log_success = torch.cumsum(log_p, dim=1)  # shape (B, D)\n",
    "#     log_fail = torch.cumsum(log_1_minus_p.flip(dims=[1]), dim=1).flip(dims=[1])  # shape (B, D)\n",
    "#\n",
    "#     # Pad with log(1) = 0 to align indexing\n",
    "#     zero = torch.zeros((B, 1), dtype=p.dtype, device=p.device)\n",
    "#     log_success = torch.cat([zero, log_success], dim=1)  # shape (B, D+1)\n",
    "#     log_fail = torch.cat([log_fail, zero], dim=1)        # shape (B, D+1)\n",
    "#\n",
    "#     # Sum log-probs for each possible cutoff (index k: first 0 after all 1s)\n",
    "#     log_probs = log_success + log_fail  # shape (B, D+1)\n",
    "#     log_probs_max = torch.max(log_probs, dim=1, keepdim=True)[0]\n",
    "#     probs_normalized = torch.exp(log_probs - log_probs_max)\n",
    "#     probs_normalized = probs_normalized / probs_normalized.sum(dim=1, keepdim=True)\n",
    "#\n",
    "#     # Sample or take the most probable index\n",
    "#     if is_sample:\n",
    "#         k = torch.multinomial(probs_normalized, num_samples=1)\n",
    "#     else:\n",
    "#         k = torch.argmax(probs_normalized, dim=1, keepdim=True)\n",
    "#\n",
    "#     # Create the monotonic sequence [1,...,1,0,...,0]\n",
    "#     arange = torch.arange(D, device=p.device).unsqueeze(0)\n",
    "#     best_sequences = (arange < k).to(p.dtype)  # shape (B, D)\n",
    "#\n",
    "#     best_probs = torch.gather(probs_normalized, dim=1, index=k).squeeze(1)\n",
    "#\n",
    "#     return best_sequences, best_probs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# most_probable_monotonic_sequence(torch.tensor([1.0, 0.2]).reshape(1, -1), False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "# class DynamicTanh(nn.Module):\n",
    "#     def __init__(self, normalized_shape, channels_last, alpha_init_value=0.5):\n",
    "#         super().__init__()\n",
    "#         self.normalized_shape = normalized_shape\n",
    "#         self.alpha_init_value = alpha_init_value\n",
    "#         self.channels_last = channels_last\n",
    "#\n",
    "#         self.alpha = nn.Parameter(torch.ones(1) * alpha_init_value)\n",
    "#         self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "#         self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = torch.tanh(self.alpha * x)\n",
    "#         if self.channels_last:\n",
    "#             x = x * self.weight + self.bias\n",
    "#         else:\n",
    "#             # x = x * self.weight[:, None, None] + self.bias[:, None, None]\n",
    "#             x = x * self.weight[:, None] + self.bias[:, None]\n",
    "#         return x\n",
    "#\n",
    "#     def extra_repr(self):\n",
    "#         return f\"normalized_shape={self.normalized_shape}, alpha_init_value={self.alpha_init_value}, channels_last={self.channels_last}\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "# from probts.model.forecaster import Forecaster\n",
    "# from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "#\n",
    "#\n",
    "# class BinConv(Forecaster):\n",
    "#     def __init__(self, context_length: int, is_prob_forecast: bool, num_bins: int, kernel_size_across_bins_2d: int = 3,\n",
    "#                  kernel_size_across_bins_1d: int = 3, num_filters_2d: int = 8,\n",
    "#                  num_filters_1d: int = 32, is_cum_sum: bool = False, num_1d_layers: int = 2, num_blocks: int = 3,\n",
    "#                  kernel_size_ffn: int = 51, dropout: float = 0.2,\n",
    "#                  **kwargs) -> None:\n",
    "#         \"\"\"\n",
    "#         Initialize the model with parameters.\n",
    "#         \"\"\"\n",
    "#         super().__init__(context_length=context_length, **kwargs)\n",
    "#         # Initialize model parameters here\n",
    "#         self.context_length = context_length\n",
    "#         self.num_bins = num_bins\n",
    "#         self.is_prob_forecast = is_prob_forecast\n",
    "#         self.num_filters_2d = num_filters_2d\n",
    "#         self.num_filters_1d = num_filters_1d\n",
    "#         self.kernel_size_across_bins_2d = kernel_size_across_bins_2d\n",
    "#         self.kernel_size_across_bins_1d = kernel_size_across_bins_1d\n",
    "#         self.is_cum_sum = is_cum_sum\n",
    "#         self.scaler = BinScaler(StandardScaler(var_specific=True), BinaryQuantizer())\n",
    "#         self.num_1d_layers = num_1d_layers\n",
    "#         self.num_blocks = num_blocks\n",
    "#         self.kernel_size_ffn = kernel_size_ffn\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         # Conv2d over (context_length, num_bins)\n",
    "#\n",
    "#         self.conv2d = nn.ModuleList([nn.Conv2d(\n",
    "#             in_channels=1,\n",
    "#             out_channels=self.num_filters_2d,\n",
    "#             # kernel_size=(context_length if i == 0 else kernel_size_across_bins_2d, kernel_size_across_bins_2d),\n",
    "#             kernel_size=(context_length, kernel_size_across_bins_2d),\n",
    "#             bias=True\n",
    "#         ) for _ in range(num_blocks)\n",
    "#         ])\n",
    "#         self.conv1d = nn.ModuleList([\n",
    "#             nn.ModuleList([\n",
    "#                 nn.Conv1d(in_channels=num_filters_2d if i == 0 else num_filters_1d,\n",
    "#                           out_channels=context_length if i == num_1d_layers - 1 else num_filters_1d,\n",
    "#                           kernel_size=kernel_size_across_bins_1d, bias=True,\n",
    "#                           groups=num_filters_1d)\n",
    "#                 # groups=1)\n",
    "#                 for i in range(num_1d_layers)\n",
    "#             ]) for _ in range(num_blocks)\n",
    "#         ])\n",
    "#         self.conv_ffn = nn.Conv1d(\n",
    "#             # in_channels=self.num_filters_1d,\n",
    "#             in_channels=context_length,\n",
    "#             out_channels=1,\n",
    "#             kernel_size=kernel_size_ffn,  # large kernel size?\n",
    "#             groups=1,\n",
    "#             bias=True\n",
    "#         )\n",
    "#         print('conv 2d:')\n",
    "#         print(self.conv2d)\n",
    "#         print('conv 1d:')\n",
    "#         print(self.conv1d)\n",
    "#         print('conv ffn:')\n",
    "#         print(self.conv_ffn)\n",
    "#         assert num_filters_2d == num_filters_1d, \"todo: change the self.act shape if not\"\n",
    "#         self.act = nn.ModuleList([\n",
    "#             nn.ModuleList([\n",
    "#                 # DynamicTanh(normalized_shape=num_filters_2d if i == 0 else num_filters_1d, channels_last=False)\n",
    "#                 DynamicTanh(normalized_shape=num_filters_2d if i < self.num_1d_layers else context_length,\n",
    "#                             channels_last=False)\n",
    "#                 for i in range(self.num_1d_layers + 1)  # applied after conv2d, and all conv1d including the last one\n",
    "#             ]) for _ in range(self.num_blocks)\n",
    "#         ])\n",
    "#\n",
    "#     def _pad_channels(self, tensor: torch.Tensor, pad_size: int, pad_val_left=1.0, pad_val_right=0.0):\n",
    "#         if pad_size == 0:\n",
    "#             return tensor\n",
    "#         left = torch.full((*tensor.shape[:-1], pad_size), pad_val_left, device=tensor.device)\n",
    "#         right = torch.full((*tensor.shape[:-1], pad_size), pad_val_right, device=tensor.device)\n",
    "#         return torch.cat([left, tensor, right], dim=-1)\n",
    "#\n",
    "#     def conv_layer(self, x: torch.Tensor, conv_func, act_func, kernel_size: int, is_2d: bool, ):\n",
    "#         # kernel_size = self.kernel_size_across_bins_2d if is_2d else self.kernel_size_across_bins_1d\n",
    "#         pad = kernel_size // 2 if kernel_size > 1 else 0\n",
    "#         x_padded = self._pad_channels(x, pad)\n",
    "#         if is_2d:\n",
    "#             x_padded = x_padded.unsqueeze(1)\n",
    "#         conv_out = conv_func(x_padded)  # (batch_size, num_filters_2d, num_bins)\n",
    "#\n",
    "#         if is_2d:\n",
    "#             conv_out = conv_out.squeeze(2)\n",
    "#         if act_func is not None:\n",
    "#             conv_out = act_func(conv_out)\n",
    "#         return conv_out\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#\n",
    "#         x = x.float()\n",
    "#         # x: (batch_size, context_length, num_bins)\n",
    "#         batch_size, context_length, num_bins = x.shape\n",
    "#         assert context_length == self.context_length, \"Mismatch in context length\"\n",
    "#\n",
    "#         for j in range(self.num_blocks):\n",
    "#\n",
    "#             residual = x\n",
    "#             x = self.conv_layer(x, self.conv2d[j], self.act[j][0], self.kernel_size_across_bins_2d, True)\n",
    "#             for i in range(self.num_1d_layers):\n",
    "#                 # x = self.conv_layer(x, self.conv1d[j][i], self.act[j][i + 1], False)\n",
    "#                 x = self.conv_layer(x, self.conv1d[j][i], F.relu,\n",
    "#                                     self.kernel_size_across_bins_1d, False)\n",
    "#             x = self.dropout(x)\n",
    "#             x = x + residual\n",
    "#\n",
    "#         out = self.conv_layer(x, self.conv_ffn, None, self.kernel_size_ffn, False).squeeze(1)\n",
    "#         # out = self.conv_ffn(x.squeeze(1))\n",
    "#\n",
    "#         if self.is_cum_sum:\n",
    "#             assert False, \"It degrades the performance\"\n",
    "#             out = torch.flip(torch.cumsum(torch.flip(out, dims=[1]), dim=1), dims=[1])\n",
    "#         return out\n",
    "#\n",
    "#     def loss(self, batch_data):\n",
    "#         \"\"\"\n",
    "#         Compute the loss for the given batch data.\n",
    "#\n",
    "#         Parameters:\n",
    "#         batch_data [dict]: Dictionary containing input data and possibly target data.\n",
    "#\n",
    "#         Returns:\n",
    "#         Tensor: Computed loss.\n",
    "#         \"\"\"\n",
    "#         # Extract inputs and targets from batch_data\n",
    "#         inputs = self.get_inputs(batch_data, 'all')\n",
    "#         orig_target = inputs[:, -self.prediction_length:, :]\n",
    "#\n",
    "#         self.scaler.fit(inputs.reshape(-1)[:-self.prediction_length])\n",
    "#         inputs = self.scaler.transform(inputs)\n",
    "#\n",
    "#         target = inputs[:, -self.prediction_length:, :]\n",
    "#         inputs = sliding_window_batch(inputs, self.context_length, self.prediction_length).float()\n",
    "#         outputs = self(inputs.view(-1, *inputs.shape[2:]))\n",
    "#         loss = F.binary_cross_entropy_with_logits(input=outputs, target=target.view(-1, *target.shape[2:]), )\n",
    "#         # print(f'loss: {loss}')\n",
    "#         return loss\n",
    "#\n",
    "#     def forecast(self, batch_data, num_samples=None):\n",
    "#         do_sample = num_samples is not None and num_samples > 1 and self.is_prob_forecast\n",
    "#\n",
    "#         inputs = self.get_inputs(batch_data, 'encode')\n",
    "#\n",
    "#         self.scaler.fit(inputs.reshape(-1))\n",
    "#         inputs = self.scaler.transform(inputs)\n",
    "#         if do_sample:\n",
    "#             inputs = repeat(inputs.unsqueeze(1), num_samples, 1)  # (B, NS, T, D)\n",
    "#             batch_size = inputs.shape[0]\n",
    "#             inputs = inputs.view(-1, *inputs.shape[2:])\n",
    "#         current_context = inputs.clone()\n",
    "#         forecasts = []\n",
    "#         for _ in range(self.prediction_length):\n",
    "#             pred = F.sigmoid(self(current_context))  # (B, D)\n",
    "#             # pred = (pred >= 0.5).int()\n",
    "#             pred, _ = most_probable_monotonic_sequence(pred, do_sample)\n",
    "#             pred = pred.int()\n",
    "#             forecasts.append(pred.unsqueeze(1))  # (B, 1, D)\n",
    "#             next_input = pred.unsqueeze(1)\n",
    "#             current_context = torch.cat([current_context[:, 1:], next_input], dim=1)\n",
    "#\n",
    "#         forecasts = torch.cat(forecasts, dim=1)\n",
    "#         forecasts = self.scaler.inverse_transform(forecasts)\n",
    "#         if do_sample:\n",
    "#             forecasts = forecasts.view(batch_size, num_samples, *forecasts.shape[1:])\n",
    "#         else:\n",
    "#             forecasts = forecasts.unsqueeze(1)  # (B, 1,  T, D)\n",
    "#         return forecasts\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "72"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_manager.context_length"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv 2d layers:\n",
      "ModuleList(\n",
      "  (0): Conv2d(1, 36, kernel_size=(72, 7), stride=(1, 1))\n",
      ")\n",
      "conv 1d layers:\n",
      "ModuleList(\n",
      "  (0): ModuleList(\n",
      "    (0): Conv1d(36, 36, kernel_size=(11,), stride=(1,), groups=36)\n",
      "    (1): Conv1d(36, 72, kernel_size=(11,), stride=(1,), groups=36)\n",
      "  )\n",
      ")\n",
      "conv ffn layer:\n",
      "Conv1d(72, 1, kernel_size=(51,), stride=(1,))\n",
      "sampling_weight_scheme: none\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreichernov/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'forecaster' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['forecaster'])`.\n"
     ]
    }
   ],
   "source": [
    "forecaster = BinConv(\n",
    "    num_bins=200,\n",
    "    is_prob_forecast=False,\n",
    "    scaler_type='standard',\n",
    "    kernel_size_across_bins_2d=7,\n",
    "    kernel_size_across_bins_1d=11,\n",
    "    num_filters_2d=72 // 2,\n",
    "    num_filters_1d=72 // 2,\n",
    "    kernel_size_ffn=200 // 4 + 1,\n",
    "    num_1d_layers=2,\n",
    "    num_blocks=1,\n",
    "    use_lags=False,\n",
    "    use_feat_idx_emb=False,\n",
    "    use_time_feat=False,\n",
    "    target_dim=data_manager.target_dim,\n",
    "    context_length=data_manager.context_length,\n",
    "    prediction_length=data_manager.prediction_length,\n",
    "    freq=data_manager.freq,\n",
    "    lags_list=data_manager.lags_list,\n",
    "    time_feat_dim=data_manager.time_feat_dim,\n",
    "    dataset=data_manager.dataset,\n",
    ")\n",
    "model = ProbTSForecastModule(\n",
    "    forecaster=forecaster,\n",
    "    scaler=data_manager.scaler,\n",
    "    learning_rate=0.001,\n",
    "    quantiles_num=20,\n",
    "    num_samples=100\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/andreichernov/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    devices=1,\n",
    "    strategy=\"auto\",\n",
    "    max_epochs=50,\n",
    "    use_distributed_sampler=False,\n",
    "    limit_train_batches=100,\n",
    "    log_every_n_steps=1,\n",
    "    accumulate_grad_batches=8,\n",
    "    default_root_dir='./results',\n",
    "    logger=CSVLogger('./logs'),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "target = []\n",
    "for data in list(data_manager.dataset_raw.train):\n",
    "    target.append(data['target'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# model.scaler.fit(torch.tensor(np.concatenate(target)).reshape(-1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "# model.scaler.standard.mean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name       | Type    | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | forecaster | BinConv | 23.4 K | train\n",
      "-----------------------------------------------\n",
      "23.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "23.4 K    Total params\n",
      "0.094     Total estimated model params size (MB)\n",
      "14        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f7e7f9617f14803aea885468cb01bd2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreichernov/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/andreichernov/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:106: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "293f7953fac143b1a52870e35696b5f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 28199) is killed by signal: Interrupt: 2. ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    824\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    825\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[108], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_dataloader\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:561\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    559\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    560\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshould_stop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m--> 561\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    562\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    563\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:48\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     47\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 48\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[1;32m     51\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:599\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    592\u001B[0m     download_model_from_registry(ckpt_path, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    593\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[1;32m    594\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    595\u001B[0m     ckpt_path,\n\u001B[1;32m    596\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    597\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    598\u001B[0m )\n\u001B[0;32m--> 599\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    601\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    602\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1012\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m   1007\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[1;32m   1009\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m   1010\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[1;32m   1011\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m-> 1012\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1014\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m   1017\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1056\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1054\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_sanity_check()\n\u001B[1;32m   1055\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m-> 1056\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1057\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1058\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnexpected state \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:216\u001B[0m, in \u001B[0;36m_FitLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start()\n\u001B[0;32m--> 216\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    218\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:455\u001B[0m, in \u001B[0;36m_FitLoop.advance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    453\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 455\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:150\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.run\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone:\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end(data_fetcher)\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:320\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.advance\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    318\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mautomatic_optimization:\n\u001B[1;32m    319\u001B[0m         \u001B[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001B[39;00m\n\u001B[0;32m--> 320\u001B[0m         batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautomatic_optimization\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    322\u001B[0m         batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_optimization\u001B[38;5;241m.\u001B[39mrun(kwargs)\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:185\u001B[0m, in \u001B[0;36m_AutomaticOptimization.run\u001B[0;34m(self, optimizer, batch_idx, kwargs)\u001B[0m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# when the strategy handles accumulation, we want to always call the optimizer step\u001B[39;00m\n\u001B[1;32m    176\u001B[0m     \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mhandles_gradient_accumulation \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39m_should_accumulate()\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;66;03m# -------------------\u001B[39;00m\n\u001B[1;32m    183\u001B[0m     \u001B[38;5;66;03m# automatic_optimization=True: perform ddp sync only when performing optimizer_step\u001B[39;00m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _block_parallel_sync_behavior(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mstrategy, block\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m--> 185\u001B[0m         \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;66;03m# BACKWARD PASS\u001B[39;00m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    190\u001B[0m \u001B[38;5;66;03m# gradient update with accumulated gradients\u001B[39;00m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    192\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step(batch_idx, closure)\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:146\u001B[0m, in \u001B[0;36mClosure.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[Tensor]:\n\u001B[0;32m--> 146\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\u001B[38;5;241m.\u001B[39mloss\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:140\u001B[0m, in \u001B[0;36mClosure.closure\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_zero_grad_fn()\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m step_output\u001B[38;5;241m.\u001B[39mclosure_loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 140\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backward_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep_output\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclosure_loss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m step_output\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:241\u001B[0m, in \u001B[0;36m_AutomaticOptimization._make_backward_fn.<locals>.backward_fn\u001B[0;34m(loss)\u001B[0m\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mbackward_fn\u001B[39m(loss: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 241\u001B[0m     \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbackward\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:328\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[0;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m    325\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 328\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m    331\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:213\u001B[0m, in \u001B[0;36mStrategy.backward\u001B[0;34m(self, closure_loss, optimizer, *args, **kwargs)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    211\u001B[0m closure_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecision_plugin\u001B[38;5;241m.\u001B[39mpre_backward(closure_loss, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module)\n\u001B[0;32m--> 213\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprecision_plugin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    215\u001B[0m closure_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecision_plugin\u001B[38;5;241m.\u001B[39mpost_backward(closure_loss, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module)\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost_backward(closure_loss)\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py:73\u001B[0m, in \u001B[0;36mPrecision.backward\u001B[0;34m(self, tensor, model, optimizer, *args, **kwargs)\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mbackward\u001B[39m(  \u001B[38;5;66;03m# type: ignore[override]\u001B[39;00m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m     61\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     62\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Performs the actual backpropagation.\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \n\u001B[1;32m     64\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     71\u001B[0m \n\u001B[1;32m     72\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/lightning/pytorch/core/module.py:1097\u001B[0m, in \u001B[0;36mLightningModule.backward\u001B[0;34m(self, loss, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1095\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fabric\u001B[38;5;241m.\u001B[39mbackward(loss, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1096\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1097\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/torch/_tensor.py:626\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    616\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    617\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    618\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    619\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    624\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    625\u001B[0m     )\n\u001B[0;32m--> 626\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    627\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    628\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    821\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    824\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    825\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "File \u001B[0;32m~/miniforge3/envs/probts/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001B[0m, in \u001B[0;36m_set_SIGCHLD_handler.<locals>.handler\u001B[0;34m(signum, frame)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mhandler\u001B[39m(signum, frame):\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001B[39;00m\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;66;03m# Python can still get and update the process status successfully.\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m     \u001B[43m_error_if_any_worker_fails\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m previous_handler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     75\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(previous_handler)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: DataLoader worker (pid 28199) is killed by signal: Interrupt: 2. "
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('logs/lightning_logs/version_230/metrics.csv')\n",
    "df.groupby('epoch').agg({'train_loss': 'mean'})\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### TODO: what does individual mean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 0\n",
    "for test_batch in test_dataloader:\n",
    "    if i > 0:\n",
    "        break\n",
    "    print(i)\n",
    "    i += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_batch['past_target_cdf'].shape\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_data = ProbTSBatchData(test_batch, model.device)\n",
    "past_target_cdf = model.scaler.transform(batch_data.past_target_cdf)\n",
    "future_target_cdf = model.scaler.transform(batch_data.future_target_cdf)\n",
    "batch_data.past_target_cdf = past_target_cdf\n",
    "\n",
    "batch_idx = 0\n",
    "with torch.no_grad():\n",
    "    prediction = model.forecaster.forecast(batch_data, num_samples=5)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction  #.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_data.past_target_cdf[0][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.scaler.inverse_transform(prediction)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.scaler.inverse_transform(prediction)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.scaler.inverse_transform(future_target_cdf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_data.future_target_cdf.reshape(-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(model.scaler.inverse_transform(prediction).reshape(-1), c='r')\n",
    "plt.plot(batch_data.future_target_cdf.reshape(-1), c='b')\n",
    "plt.plot(model.scaler.inverse_transform(future_target_cdf).reshape(-1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_data.past_target_cdf.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# context_length = data_manager.context_length\n",
    "# prediction_length = data_manager.prediction_length\n",
    "# past_range = range(0, context_length)\n",
    "# future_range = range(context_length, context_length + prediction_length)\n",
    "# full_range = range(0, context_length + prediction_length)\n",
    "#\n",
    "# for i in range(min(10, forecaster.target_dim)):\n",
    "#     target = torch.cat([past_target_cdf[batch_idx, -context_length:, i], future_target_cdf[batch_idx, :, i]])\n",
    "#     plt.figure(figsize=(10, 2))\n",
    "#     plt.plot(full_range, target)\n",
    "#     plt.plot(future_range, prediction[:, i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "% % time\n",
    "trainer.test(model=model, datamodule=data_module);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "future_target_cdf.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trainer.test(model=model, dataloaders=train_dataloader);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from probts.utils import repeat\n",
    "\n",
    "repeat(torch.rand(5, 2).unsqueeze(1), 10, 1).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
